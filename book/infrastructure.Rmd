# Infrastructure

- Start with building the right infrastructure, on-premise or cloud
- This is one of the greater challenges. If you're working on a greenfield project, to build the pipeline to get to the point where you can actually start doing DS
- Big challenges:
  - 80%: Get the data ready to do DS on it
    - won't change soon: Data is messy and different data sets don't align
    - But companies are pushing towards better **data governance**
  - Tool selection
  - Best practices: What are other companies in this industry doing?
  
## Cloud Services

- AWS, Azure, GCP
- Good way to train large models
- cloud makes many things easier, but new problems arise, e.g. data leaks (horror stories of accidentally public S3 containers)
- Nice way to get familiar with the concepts: I like doing a certificate as a clearly defined goal line to get familiar with what's possible

## Pipelines, APIs and Software Engineering

- At this stage, you'll also need skills that are valuable in the DevOps part (productionizing your model)

## Scalability

- Ideally, have a dynamically scalable system
- Case study: During the Corona crisis, everyone suddenly played poker home games online, and a certain home game poker app crashed under the unexpectedly heavy traffic
- Amazon ECR (or was it ECS?) automatically scales Docker containers and is a good solution for this problem.

## Serverless

Serverless: Ich denke das ist in Cloud Services inkludiert. Aber hier würde ich persönlich mir vor allem einen großen Serverless Part erwarten. Wie man z.B. mit serverless services eine ML App (zum Beispiel) aufzieht. Angefangen von simplen lambda Funktionen zu Services wie sagemaker, dynamo, glue, athena, etc. Vor allem die Verbindung von großen Datenmengen zum einfachen aggregieren bis zum trainieren von Modellen. 
